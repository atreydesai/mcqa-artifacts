"""
Abstract base class implementation of a DataFetcher. Uses a specified preference dataset and experiment to obtain a list of data inputs to use for the prompt.
"""

from enums import PromptType
import json
import os
import datasets
import re
from pathlib import Path
from datasets.utils.logging import disable_progress_bar
from typing import Any
disable_progress_bar()

from abc import ABC, abstractmethod

class DataFetcher(ABC):
    @abstractmethod
    def get_data(self):
        """Retrieve data from the source."""
        """Output: List of keyword arguments in dictionary form"""
        pass

    def collect_files(self, keywords, res_dir):
        """Collect all files with valid keywords"""
        valid_files = []
        directory = Path(res_dir)
        for file in directory.rglob('*'):
            if file.is_file():
                valid = True
                for substr in keywords:
                    if substr.strip() not in str(file):
                        valid = False
                        break
                if valid:
                    valid_files.append(file)
        return valid_files

class PromptResponseFetcher(DataFetcher):
    """Loads the chosen and rejected responses for the model to generate personas for"""

    def __init__(self, ds_name: str, split_name: str):
        self.ds = self.load_hf_dataset(ds_name, split_name)

    def load_hf_dataset(self, ds_name: str, split_name: str) -> Any:
        if os.path.isfile(ds_name):
            ds = datasets.load_from_disk(ds_name, split_name)
        else:
            ds = datasets.load_dataset(ds_name, split_name)
        return ds['train']
            
    def get_data(self):
        prompts, r1, r2 = self.ds['prompt'], self.ds['chosen'], self.ds['rejected']
        all_prompts, all_r1, all_r2 = prompts + prompts, r1 + r2, r2 + r1 # swap orders of chosen + rejected responses
        return [{'prompt': p, 'chosen': c, 'rejected': r} for p, c, r in list(zip(all_prompts, all_r1, all_r2))]

class PersonaAccuracyFetcher(DataFetcher):
    """Loads the personas generated by the models to allow a singular judge to evaluate accuracy"""

    def __init__(self, ds_name: str, split_name: str, run_name: str, res_dir: str, prompt_type: PromptType, evalulator_model_name: str):
        
        # obtain results from all models that have been run on the same split/run_name/prompt_type for the judge
        required_substrings = [split_name, run_name, 'persona_inference.jsonl']
        valid_files = self.collect_files(keywords=required_substrings, res_dir=res_dir)

        personas = []
        prompts = []
        r1 = []
        r2 = []
        gold_data = []
        
        for file in valid_files:
            file = str(file)
            model_name = file[file.index(res_dir) + len(res_dir):]
            model_name = model_name.split('/')[0].strip() # clean model name
            
            with open(file, 'r') as f:
                lines = f.readlines()
                for line_idx, line in enumerate(lines):
                    json_data = json.loads(line)
                    persona = json_data['raw_text']  # persona was generated by the model
                    if persona == None:
                        continue
                    
                    # extract the prompt (query), chosen response, rejected response, and persona from the prompt template
                    match = re.match(r"Prompt:\s*(?P<p>.*)\n---\nChosen Response:\s*(?P<c>.*)\n---\nRejected Response:\s*(?P<r>.*)\n---\nPersona:", json_data['prompt'], re.DOTALL)
                    if match:
                        p = match.group('p').strip()
                        c = match.group('c').strip()
                        r = match.group('r').strip()
                        for order in [(c, r, 1), (r, c, 2)]: # collect both orders for judge position bias
                            r1_, r2_, l = order
                            prompts.append(p)
                            r1.append(r1_)
                            r2.append(r2_)
                            personas.append(persona)

                            gold_data.append({'model_name': model_name, 'label': l, 'is_chosen': line_idx < len(lines) // 2}) # first half are chosen personas, second half are rejected

        # save the gold labels of what the judge is about to run
        with open(f'{res_dir}{evalulator_model_name}/{split_name}/{run_name}/persona_accuracy_key.jsonl', 'w+') as handle:
            for output in gold_data:
                json.dump(output, handle)
                handle.write('\n')

        self.data = [{'prompt': p, 'r1': c, 'r2': r, 'persona': pers} for p, c, r, pers in list(zip(prompts, r1, r2, personas))]

    def get_data(self):
        return self.data

class PreferencePersonasFetcher(DataFetcher):
    """Loads data where a model judges its own personas"""

    def __init__(self, ds_name: str, split_name: str, run_name: str, res_dir: str, prompt_type: PromptType, evalulator_model_name: str):
        
        # Define the file where the models personas are
        required_substrings = [f'{res_dir}/{evalulator_model_name}/{split_name}/{run_name}/persona_inference.jsonl']
        valid_files = self.collect_files(keywords=required_substrings, res_dir=res_dir)

        all_data = dict()
        prompt_order = []

        for file in valid_files:
            file = str(file)
            model_name = file[file.index(res_dir) + len(res_dir):]
            model_name = model_name.split('/')[0].strip() # clean model name
            with open(file, 'r') as f:
                lines = f.readlines()
                for line_idx, line in enumerate(lines):
                    json_data = json.loads(line)
                    persona = json_data['raw_text'] # persona was generated by the model
                    if persona == None: 
                        continue
                    persona = persona.strip().split('\n')[0].strip()

                    # extract prompt and responses from template
                    match = re.match(r"Prompt:\s*(?P<p>.*)\n---\nChosen Response:\s*(?P<c>.*)\n---\nRejected Response:\s*(?P<r>.*)\n---\nPersona:", json_data['prompt'], re.DOTALL)
                    if match:
                        p = match.group('p').strip()
                        c = match.group('c').strip()
                        r = match.group('r').strip()

                        if p not in prompt_order:
                            prompt_order.append(p)

                        # shortcut so we can get chosen/rejected personas and outputs matched to each other
                        arr = all_data.get(p, {'r1': None, 'p1': None, 'r2': None, 'p2': None})
                        if arr['r1'] == None:
                            arr['r1'] = c
                            arr['p1'] = persona
                        else:
                            arr['r2'] = c
                            arr['p2'] = persona
                        all_data[p] = arr

        # form both pairwise comparisons on which persona is better (position bias)
        prompts = []
        r1 = []
        r2 = []
        personas = []

        for p in prompt_order:
            arr = all_data[p]

            prompts.append(p)
            r1.append(arr['r1'])
            r2.append(arr['r2'])
            personas.append((arr['p1'], arr['p2']))

            prompts.append(p)
            r1.append(arr['r2'])
            r2.append(arr['r1'])
            personas.append((arr['p2'], arr['p1']))

        self.data = [{'prompt': p, 'r1': c, 'r2': r, 'persona1': pers[0], 'persona2': pers[1]} for p, c, r, pers in list(zip(prompts, r1, r2, personas))]

    def get_data(self):
        return self.data

class DataFetcherFactory:

    @staticmethod
    def get_data_fetcher(prompt_type: PromptType, args: Any):
        if prompt_type in {PromptType.persona_inference}:
            return PromptResponseFetcher(ds_name=args.dataset_name, split_name=args.inference_split)
        elif prompt_type in {PromptType.persona_accuracy}:
            return PersonaAccuracyFetcher(ds_name=args.dataset_name, split_name=args.inference_split, run_name=args.run_name, res_dir=args.res_dir, prompt_type=prompt_type, evalulator_model_name=args.model_name)
        elif prompt_type in {PromptType.persona_prefs}:
            return PreferencePersonasFetcher(ds_name=args.dataset_name, split_name=args.inference_split, run_name=args.run_name, res_dir=args.res_dir, prompt_type=prompt_type, evalulator_model_name=args.model_name)
        else:
            raise ValueError(f"Unsupported DataFetcher type: {prompt_type}")